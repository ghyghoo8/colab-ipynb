{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依赖安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "!pip install protobuf==3.20.0 transformers==4.27.1 icetk cpm_kernels torch\n",
    "!pip install fastapi pydantic uvicorn sse_starlette pyngrok nest-asyncio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git lfs install\n",
    "git clone https://huggingface.co/THUDM/chatglm-6b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available() == False:\n",
    "    print(\"请在右上方 Colab 运行时类型中，选择 GPU 类型的运行时。\")\n",
    "\n",
    "chatglm_models = [\n",
    "    \"THUDM/chatglm-6b\",       # 原始模型\n",
    "    \"THUDM/chatglm-6b-int8\",  # int8 量化\n",
    "    \"THUDM/chatglm-6b-int4\",  # int4 量化\n",
    "]\n",
    "\n",
    "CHATGLM_MODEL = \"THUDM/chatglm-6b-int4\"\n",
    "\n",
    "# GPU/CPU\n",
    "RUNNING_DEVICE = \"GPU\"\n",
    "\n",
    "# API_TOKEN\n",
    "TOKEN = \"token1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def init_chatglm(model_name: str, running_device: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    if running_device == \"GPU\":\n",
    "        model = model.half().cuda()\n",
    "    else:\n",
    "        model = model.float()\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = init_chatglm(CHATGLM_MODEL, RUNNING_DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response)\n",
    "print(history)\n",
    "response, history = model.chat(tokenizer, \"很高兴认识你\", history=history)\n",
    "print(response)\n",
    "print(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载webui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/ninehills/chatgpt-web/releases/download/1.0/dist.tgz -O dist.tgz\n",
    "!tar zxvf dist.tgz\n",
    "!mv ./dist/index.html ./dist/assets/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启动服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fastapi import FastAPI, Request, status, HTTPException\n",
    "from fastapi.responses import JSONResponse, HTMLResponse\n",
    "from pydantic import BaseModel\n",
    "from sse_starlette.sse import EventSourceResponse\n",
    "from fastapi.responses import RedirectResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# 参考 https://github.com/josStorer/selfhostedAI/blob/master/main.py\n",
    "\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():\n",
    "        with torch.cuda.device(0):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")\n",
    "\n",
    "app.mount(\"/assets\", StaticFiles(directory=\"dist/assets\"), name=\"assets\")\n",
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class Body(BaseModel):\n",
    "    messages: List[Message]\n",
    "    model: str\n",
    "    stream: Optional[bool] = False\n",
    "    max_tokens: Optional[int] = 256\n",
    "    temperature: Optional[float] = 0.95\n",
    "    top_p: Optional[float] = 0.7\n",
    "\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return RedirectResponse(\"/assets/index.html\")\n",
    "\n",
    "\n",
    "@app.get(\"/v1/models\")\n",
    "def get_models():\n",
    "    return {\"data\": [\n",
    "      {\n",
    "        \"created\": 1677610602,\n",
    "        \"id\": \"gpt-3.5-turbo\",\n",
    "        \"object\": \"model\",\n",
    "        \"owned_by\": \"openai\",\n",
    "        \"permission\": [\n",
    "          {\n",
    "            \"created\": 1680818747,\n",
    "            \"id\": \"modelperm-fTUZTbzFp7uLLTeMSo9ks6oT\",\n",
    "            \"object\": \"model_permission\",\n",
    "            \"allow_create_engine\": False,\n",
    "            \"allow_sampling\": True,\n",
    "            \"allow_logprobs\": True,\n",
    "            \"allow_search_indices\": False,\n",
    "            \"allow_view\": True,\n",
    "            \"allow_fine_tuning\": False,\n",
    "            \"organization\": \"*\",\n",
    "            \"group\": None,\n",
    "            \"is_blocking\": False\n",
    "          }\n",
    "        ],\n",
    "        \"root\": \"gpt-3.5-turbo\",\n",
    "        \"parent\": None,\n",
    "      },\n",
    "    ],\n",
    "    \"object\": \"list\"\n",
    "  }\n",
    "\n",
    "def generate_response(content: str):\n",
    "    return {\n",
    "        \"id\": \"chatcmpl-77PZm95TtxE0oYLRx3cxa6HtIDI7s\",\n",
    "        \"object\": \"chat.completion\",\n",
    "        \"created\": 1682000966,\n",
    "        \"model\": \"gpt-3.5-turbo-0301\",\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": 10,\n",
    "            \"completion_tokens\": 10,\n",
    "            \"total_tokens\": 20,\n",
    "        },\n",
    "        \"choices\": [{\n",
    "            \"message\": {\"role\": \"assistant\", \"content\": content}, \"finish_reason\": \"stop\", \"index\": 0}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def generate_stream_response_start():\n",
    "    return {\"id\":\"chatcmpl-77QWpn5cxFi9sVMw56DZReDiGKmcB\",\"object\":\"chat.completion.chunk\",\"created\":1682004627,\"model\":\"gpt-3.5-turbo-0301\",\"choices\":[{\"delta\":{\"role\":\"assistant\"},\"index\":0,\"finish_reason\":None}]}\n",
    "\n",
    "def generate_stream_response(content: str):\n",
    "    return {\n",
    "        \"id\":\"chatcmpl-77QWpn5cxFi9sVMw56DZReDiGKmcB\",\n",
    "        \"object\":\"chat.completion.chunk\",\n",
    "        \"created\":1682004627,\n",
    "        \"model\":\"gpt-3.5-turbo-0301\",\n",
    "        \"choices\":[{\"delta\":{\"content\":content},\"index\":0,\"finish_reason\":None}\n",
    "    ]}\n",
    "\n",
    "def generate_stream_response_stop():\n",
    "    return {\"id\":\"chatcmpl-77QWpn5cxFi9sVMw56DZReDiGKmcB\",\"object\":\"chat.completion.chunk\",\"created\":1682004627,\"model\":\"gpt-3.5-turbo-0301\",\"choices\":[{\"delta\":{},\"index\":0,\"finish_reason\":\"stop\"}]}\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def completions(body: Body, request: Request):\n",
    "    # Cancel token\n",
    "    #if request.headers.get(\"Authorization\").split(\" \")[1] != TOKEN:\n",
    "    #    raise HTTPException(status.HTTP_401_UNAUTHORIZED, \"Token is wrong!\")\n",
    "    \n",
    "    torch_gc()\n",
    "\n",
    "    question = body.messages[-1]\n",
    "    if question.role == 'user':\n",
    "        question = question.content\n",
    "    else:\n",
    "        raise HTTPException(status.HTTP_400_BAD_REQUEST, \"No Question Found\")\n",
    "\n",
    "    history = []\n",
    "    user_question = ''\n",
    "    for message in body.messages:\n",
    "        if message.role == 'system':\n",
    "            history.append((message.content, \"OK\"))\n",
    "        if message.role == 'user':\n",
    "            user_question = message.content\n",
    "        elif message.role == 'assistant':\n",
    "            assistant_answer = message.content\n",
    "            history.append((user_question, assistant_answer))\n",
    "\n",
    "    print(f\"question = {question}, history = {history}\")\n",
    "\n",
    "    \n",
    "    if body.stream:\n",
    "        async def eval_chatglm():\n",
    "            sends = 0\n",
    "            first = True\n",
    "            for response, _ in model.stream_chat(\n",
    "                tokenizer, question, history,\n",
    "                temperature=body.temperature,\n",
    "                top_p=body.top_p,\n",
    "                max_length=max(2048, body.max_tokens)):\n",
    "                if await request.is_disconnected():\n",
    "                    return\n",
    "                ret = response[sends:]\n",
    "                sends = len(response)\n",
    "                if first:\n",
    "                    first = False\n",
    "                    yield json.dumps(generate_stream_response_start(), ensure_ascii=False)\n",
    "                yield json.dumps(generate_stream_response(ret), ensure_ascii=False)\n",
    "            yield json.dumps(generate_stream_response_stop(), ensure_ascii=False)\n",
    "            yield \"[DONE]\"\n",
    "        return EventSourceResponse(eval_chatglm(), ping=10000)\n",
    "    else:\n",
    "        response, _ = model.chat(\n",
    "            tokenizer, question, history,\n",
    "            temperature=body.temperature,\n",
    "            top_p=body.top_p,\n",
    "            max_length=max(2048, body.max_tokens))\n",
    "        print(f\"response: {response}\")\n",
    "        return JSONResponse(content=generate_response(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
    "!chmod a+x cloudflared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "启动ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 Notebook 中运行所需\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#from pyngrok import ngrok, conf\n",
    "\n",
    "#ngrok.set_auth_token(os.environ[\"ngrok_token\"])\n",
    "#http_tunnel = ngrok.connect(8000)\n",
    "#print(http_tunnel.public_url)\n",
    "\n",
    "import subprocess\n",
    "print(\"start cloudflared runnel\")\n",
    "f = open(\"stdout\", \"w\")\n",
    "p = subprocess.Popen(['./cloudflared', '--url', 'http://localhost:8000'], bufsize=0, stdout=f, stderr=subprocess.STDOUT)\n",
    "\n",
    "import time\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "!grep -F trycloudflare stdout\n",
    "\n",
    "print(\"start app\")\n",
    "import uvicorn\n",
    "uvicorn.run(app, port=8000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```from: https://github.com/ninehills/chatglm-openai-api```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
