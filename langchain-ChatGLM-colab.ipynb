{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ghyghoo8/colab-ipynb/blob/main/langchain-ChatGLM-colab.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git /content/langchain-ChatGLM\n",
    "\n",
    "%cd /content/langchain-ChatGLM\n",
    "!pip install -r /content/langchain-ChatGLM/requirements.txt\n",
    "\n",
    "# ‰∏ãËΩΩÊ®°Âûã\n",
    "!git clone https://huggingface.co/THUDM/chatglm-6b /content/langchain-ChatGLM/content/chatglm-6b\n",
    "!git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese /content/langchain-ChatGLM/content/text2vec\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÊâßË°åwebui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import shutil\n",
    "from chains.local_doc_qa import LocalDocQA\n",
    "from configs.model_config import *\n",
    "import nltk\n",
    "\n",
    "nltk.data.path = [os.path.join(os.path.dirname(__file__), \"nltk_data\")] + nltk.data.path\n",
    "\n",
    "# return top-k text chunk from vector store\n",
    "VECTOR_SEARCH_TOP_K = 6\n",
    "\n",
    "# LLM input history length\n",
    "LLM_HISTORY_LEN = 3\n",
    "\n",
    "\n",
    "def get_file_list():\n",
    "    if not os.path.exists(\"content\"):\n",
    "        return []\n",
    "    return [f for f in os.listdir(\"content\")]\n",
    "\n",
    "\n",
    "file_list = get_file_list()\n",
    "\n",
    "embedding_model_dict_list = list(embedding_model_dict.keys())\n",
    "\n",
    "llm_model_dict_list = list(llm_model_dict.keys())\n",
    "\n",
    "local_doc_qa = LocalDocQA()\n",
    "\n",
    "\n",
    "def upload_file(file):\n",
    "    if not os.path.exists(\"content\"):\n",
    "        os.mkdir(\"content\")\n",
    "    filename = os.path.basename(file.name)\n",
    "    shutil.move(file.name, \"content/\" + filename)\n",
    "    # file_listÈ¶ñ‰ΩçÊèíÂÖ•Êñ∞‰∏ä‰º†ÁöÑÊñá‰ª∂\n",
    "    file_list.insert(0, filename)\n",
    "    return gr.Dropdown.update(choices=file_list, value=filename)\n",
    "\n",
    "\n",
    "def get_answer(query, vs_path, history):\n",
    "    if vs_path:\n",
    "        resp, history = local_doc_qa.get_knowledge_based_answer(\n",
    "            query=query, vs_path=vs_path, chat_history=history)\n",
    "    else:\n",
    "        history = history + [[None, \"ËØ∑ÂÖàÂä†ËΩΩÊñá‰ª∂ÂêéÔºåÂÜçËøõË°åÊèêÈóÆ„ÄÇ\"]]\n",
    "    return history, \"\"\n",
    "\n",
    "\n",
    "def update_status(history, status):\n",
    "    history = history + [[None, status]]\n",
    "    print(status)\n",
    "    return history\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    try:\n",
    "        local_doc_qa.init_cfg()\n",
    "        return \"\"\"Ê®°ÂûãÂ∑≤ÊàêÂäüÂä†ËΩΩÔºåËØ∑ÈÄâÊã©Êñá‰ª∂ÂêéÁÇπÂáª\"Âä†ËΩΩÊñá‰ª∂\"ÊåâÈíÆ\"\"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\"Ê®°ÂûãÊú™ÊàêÂäüÂä†ËΩΩÔºåËØ∑ÈáçÊñ∞ÈÄâÊã©ÂêéÁÇπÂáª\"Âä†ËΩΩÊ®°Âûã\"ÊåâÈíÆ\"\"\"\n",
    "\n",
    "\n",
    "def reinit_model(llm_model, embedding_model, llm_history_len, top_k, history):\n",
    "    try:\n",
    "        local_doc_qa.init_cfg(llm_model=llm_model,\n",
    "                              embedding_model=embedding_model,\n",
    "                              llm_history_len=llm_history_len,\n",
    "                              top_k=top_k)\n",
    "        model_status = \"\"\"Ê®°ÂûãÂ∑≤ÊàêÂäüÈáçÊñ∞Âä†ËΩΩÔºåËØ∑ÈÄâÊã©Êñá‰ª∂ÂêéÁÇπÂáª\"Âä†ËΩΩÊñá‰ª∂\"ÊåâÈíÆ\"\"\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        model_status = \"\"\"Ê®°ÂûãÊú™ÊàêÂäüÈáçÊñ∞Âä†ËΩΩÔºåËØ∑ÈáçÊñ∞ÈÄâÊã©ÂêéÁÇπÂáª\"Âä†ËΩΩÊ®°Âûã\"ÊåâÈíÆ\"\"\"\n",
    "    return history + [[None, model_status]]\n",
    "\n",
    "\n",
    "\n",
    "def get_vector_store(filepath, history):\n",
    "    if local_doc_qa.llm and local_doc_qa.embeddings:\n",
    "        vs_path = local_doc_qa.init_knowledge_vector_store([\"content/\" + filepath])\n",
    "        if vs_path:\n",
    "            file_status = \"Êñá‰ª∂Â∑≤ÊàêÂäüÂä†ËΩΩÔºåËØ∑ÂºÄÂßãÊèêÈóÆ\"\n",
    "        else:\n",
    "            file_status = \"Êñá‰ª∂Êú™ÊàêÂäüÂä†ËΩΩÔºåËØ∑ÈáçÊñ∞‰∏ä‰º†Êñá‰ª∂\"\n",
    "    else:\n",
    "        file_status = \"Ê®°ÂûãÊú™ÂÆåÊàêÂä†ËΩΩÔºåËØ∑ÂÖàÂú®Âä†ËΩΩÊ®°ÂûãÂêéÂÜçÂØºÂÖ•Êñá‰ª∂\"\n",
    "        vs_path = None\n",
    "    return vs_path, history + [[None, file_status]]\n",
    "\n",
    "\n",
    "block_css = \"\"\".importantButton {\n",
    "    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;\n",
    "    border: none !important;\n",
    "}\n",
    "\n",
    ".importantButton:hover {\n",
    "    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;\n",
    "    border: none !important;\n",
    "}\"\"\"\n",
    "\n",
    "webui_title = \"\"\"\n",
    "# üéâlangchain-ChatGLM WebUIüéâ\n",
    "\n",
    "üëç [https://github.com/imClumsyPanda/langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "init_message = \"\"\"Ê¨¢Ëøé‰ΩøÁî® langchain-ChatGLM Web UIÔºåÂºÄÂßãÊèêÈóÆÂâçÔºåËØ∑‰æùÊ¨°Â¶Ç‰∏ã 3 ‰∏™Ê≠•È™§Ôºö\n",
    "1. ÈÄâÊã©ËØ≠Ë®ÄÊ®°Âûã„ÄÅEmbedding Ê®°ÂûãÂèäÁõ∏ÂÖ≥ÂèÇÊï∞ÂêéÁÇπÂáª\"ÈáçÊñ∞Âä†ËΩΩÊ®°Âûã\"ÔºåÂπ∂Á≠âÂæÖÂä†ËΩΩÂÆåÊàêÊèêÁ§∫\n",
    "2. ‰∏ä‰º†ÊàñÈÄâÊã©Â∑≤ÊúâÊñá‰ª∂‰Ωú‰∏∫Êú¨Âú∞Áü•ËØÜÊñáÊ°£ËæìÂÖ•ÂêéÁÇπÂáª\"ÈáçÊñ∞Âä†ËΩΩÊñáÊ°£\"ÔºåÂπ∂Á≠âÂæÖÂä†ËΩΩÂÆåÊàêÊèêÁ§∫\n",
    "3. ËæìÂÖ•Ë¶ÅÊèê‰∫§ÁöÑÈóÆÈ¢òÂêéÔºåÁÇπÂáªÂõûËΩ¶Êèê‰∫§ \"\"\"\n",
    "\n",
    "\n",
    "model_status = init_model()\n",
    "\n",
    "with gr.Blocks(css=block_css) as demo:\n",
    "    vs_path, file_status, model_status = gr.State(\"\"), gr.State(\"\"), gr.State(model_status)\n",
    "    gr.Markdown(webui_title)\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot([[None, init_message], [None, model_status.value]],\n",
    "                                 elem_id=\"chat-box\",\n",
    "                                 show_label=False).style(height=750)\n",
    "            query = gr.Textbox(show_label=False,\n",
    "                               placeholder=\"ËØ∑ËæìÂÖ•ÊèêÈóÆÂÜÖÂÆπÔºåÊåâÂõûËΩ¶ËøõË°åÊèê‰∫§\",\n",
    "                               ).style(container=False)\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            llm_model = gr.Radio(llm_model_dict_list,\n",
    "                                 label=\"LLM Ê®°Âûã\",\n",
    "                                 value=LLM_MODEL,\n",
    "                                 interactive=True)\n",
    "            llm_history_len = gr.Slider(0,\n",
    "                                        10,\n",
    "                                        value=LLM_HISTORY_LEN,\n",
    "                                        step=1,\n",
    "                                        label=\"LLM history len\",\n",
    "                                        interactive=True)\n",
    "            embedding_model = gr.Radio(embedding_model_dict_list,\n",
    "                                       label=\"Embedding Ê®°Âûã\",\n",
    "                                       value=EMBEDDING_MODEL,\n",
    "                                       interactive=True)\n",
    "            top_k = gr.Slider(1,\n",
    "                              20,\n",
    "                              value=VECTOR_SEARCH_TOP_K,\n",
    "                              step=1,\n",
    "                              label=\"ÂêëÈáèÂåπÈÖç top k\",\n",
    "                              interactive=True)\n",
    "            load_model_button = gr.Button(\"ÈáçÊñ∞Âä†ËΩΩÊ®°Âûã\")\n",
    "\n",
    "            # with gr.Column():\n",
    "            with gr.Tab(\"select\"):\n",
    "                selectFile = gr.Dropdown(file_list,\n",
    "                                         label=\"content file\",\n",
    "                                         interactive=True,\n",
    "                                         value=file_list[0] if len(file_list) > 0 else None)\n",
    "            with gr.Tab(\"upload\"):\n",
    "                file = gr.File(label=\"content file\",\n",
    "                               file_types=['.txt', '.md', '.docx', '.pdf']\n",
    "                               )  # .style(height=100)\n",
    "            load_file_button = gr.Button(\"Âä†ËΩΩÊñá‰ª∂\")\n",
    "    load_model_button.click(reinit_model,\n",
    "                            show_progress=True,\n",
    "                            inputs=[llm_model, embedding_model, llm_history_len, top_k, chatbot],\n",
    "                            outputs=chatbot\n",
    "                            )\n",
    "    # Â∞Ü‰∏ä‰º†ÁöÑÊñá‰ª∂‰øùÂ≠òÂà∞contentÊñá‰ª∂Â§π‰∏ã,Âπ∂Êõ¥Êñ∞‰∏ãÊãâÊ°Ü\n",
    "    file.upload(upload_file,\n",
    "                inputs=file,\n",
    "                outputs=selectFile)\n",
    "    load_file_button.click(get_vector_store,\n",
    "                           show_progress=True,\n",
    "                           inputs=[selectFile, chatbot],\n",
    "                           outputs=[vs_path, chatbot],\n",
    "                           )\n",
    "    query.submit(get_answer,\n",
    "                 [query, vs_path, chatbot],\n",
    "                 [chatbot, query],\n",
    "                 )\n",
    "\n",
    "demo.queue(concurrency_count=3).launch(\n",
    "    server_name='0.0.0.0', share=True, inbrowser=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
